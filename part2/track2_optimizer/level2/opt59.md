## ex59: Tokenize and Flatten Supplier Comments for Text Mining

> **Type:** Stretch | **Track:** Query Optimizer  
>
> **Difficulty:** 7 / 10

### Business context
Your team is exploring **automated topic modeling and text mining** on supplier comments. As a first step, you need to prepare a normalized table of **individual words** extracted from the `S_COMMENT` field of the `SUPPLIER` table.

These comment strings are semi-structured ‚Äî often containing punctuation, filler words, and varying casing.  
To build a tokenized layer for downstream ML and search use cases, your goal is to:

* Lowercase the comments
* Strip punctuation
* Split comments into **individual words**
* Output one word per row, linked to its original supplier

---

### Starter query

```sql
-- Peek at the kind of text we‚Äôll process
SELECT S_SUPPKEY, S_NAME, S_COMMENT
FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.SUPPLIER
LIMIT 10;
```

---

### Required datasets

* `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.SUPPLIER`

<details>
<summary>üí° Hint (click to expand)</summary>

#### How to think about it

* First normalize: lowercase, remove punctuation using `REGEXP_REPLACE`
* Then split using `SPLIT` to turn a sentence into an array of words
* Finally use `FLATTEN` to explode that array into individual rows

#### Helpful SQL concepts

`REGEXP_REPLACE`, `LOWER`, `SPLIT`, `FLATTEN`, `LATERAL`

```sql
-- Removes all non-alphanumeric characters (except space)
REGEXP_REPLACE(S_COMMENT, '[^a-zA-Z0-9 ]', '')
```

</details>

<details>
<summary>‚úÖ Solution (click to expand)</summary>

```sql
WITH normalized_comments AS (
  SELECT
    S_SUPPKEY,
    LOWER(REGEXP_REPLACE(S_COMMENT, '[^a-zA-Z0-9 ]', '')) AS clean_comment
  FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.SUPPLIER
),
tokenized_words AS (
  SELECT
    S_SUPPKEY,
    f.value::string AS word
  FROM normalized_comments,
       LATERAL FLATTEN(input => SPLIT(clean_comment, ' ')) f
)
SELECT * FROM tokenized_words
WHERE LENGTH(word) > 2
ORDER BY S_SUPPKEY, word;
```

#### Why this works

- `REGEXP_REPLACE` strips punctuation while preserving word boundaries
- `LOWER()` standardizes casing for easier matching
- `SPLIT()` breaks strings into word arrays
- `FLATTEN()` explodes those arrays into tidy row-based tokens

This yields a powerful foundation for **search**, **clustering**, or **natural language processing** ‚Äî without relying on external tools.

#### Business answer

Each supplier‚Äôs comment is now tokenized into individual lowercase words ‚Äî stored row-wise for use in modeling or search pipelines.

#### Take-aways

* REGEXP and string functions are a powerful part of the SQL toolkit
* Preprocessing text before tokenization is critical for quality output
* `FLATTEN` lets you work with arrays in tabular fashion
* Avoid including empty strings or short tokens using `WHERE LENGTH(word) > 2`

</details>

<details>
<summary>üéÅ Bonus Exercise (click to expand)</summary>

Can you figure out what the **top 10 most common words** across all supplier comments are?

Group the resulting words and use `COUNT(DISTINCT S_SUPPKEY)` to find how many different suppliers used each word.

</details>
